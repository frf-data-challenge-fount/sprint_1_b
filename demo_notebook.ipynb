{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff2f54bc-20a3-433e-8225-6f79d03c87be",
   "metadata": {},
   "source": [
    "# Exploring ALS and TLS Point Clouds\n",
    "\n",
    "At the conclusion of this module, you will be able to:\n",
    "\n",
    "1. Discover how ALS and TLS datasets are captured and their real-world applications in forestry.\n",
    "2. Identify the structure of ALS and TLS datasets, including key attributes like coordinates, intensity, and color (RGB).\n",
    "3. Create visualizations and perform basic machine learning tasks using ALS and TLS datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2a4002-66ca-4acc-bd11-1ef524fa033c",
   "metadata": {},
   "source": [
    "## ALS Data\n",
    "\n",
    "Airborne Laser Scanning (ALS) is a LiDAR technique that uses sensors mounted on aircraft or drones to collect data over large areas. ALS is widely used in applications such as forestry, topographic mapping and urban planning due to its ability to capture high-resolution 3D data over large landscapes. The data collected typically includes coordinates (X, Y, Z), intensity (reflection of the laser pulse) and sometimes RGB values. ALS is particularly effective for analysing terrain, canopy structure and vegetation density because it can penetrate through gaps in the canopy to produce detailed elevation models.\n",
    "\n",
    "## Our Data\n",
    "The data we’re working with is in the LAZ file format, which is a compressed version of the LAS format. If you want to dive deeper into this format, check out the [**American Society for Photogrammetry and Remote Sensing (ASPRS)** site](https://www.asprs.org/divisions-committees/lidar-division/laser-las-file-format-exchange-activities). \n",
    "\n",
    "Each file is about 150MB, and since the data is already hosted in an open cloud, downloading it to our server isn’t the most efficient option. Instead, we’ll load the data directly into memory for processing.\n",
    "\n",
    "To work with this data, we'll use the [laspy](https://laspy.readthedocs.io/en/latest/index.html) library, a powerful tool for handling LAS and LAZ files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a117df-ded0-48a6-8a37-a86e8a6af776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import laspy\n",
    "from io import BytesIO\n",
    "\n",
    "# 1. We set the URL for ALS file\n",
    "als_url = \"https://wifire-data.sdsc.edu/nc/s/nnSMqWfZAN6Cz6m/download?path=%2Fals_data&files=USGS_LPC_CA_SierraNevada_B22_10SGJ3270.laz\"\n",
    "\n",
    "# 2. We fetch ALS file content\n",
    "response_als = requests.get(als_url)\n",
    "\n",
    "# 3. We load the ALS data into memory\n",
    "als_buffer = BytesIO(response_als.content)\n",
    "\n",
    "# 4. We open the file using laspy, and print the format and number of points.\n",
    "with laspy.open(als_buffer) as als_file:\n",
    "    print(f\"ALS Point Format: {als_file.header.point_format}\")\n",
    "    print(f\"ALS Number of Points: {als_file.header.point_count}\")\n",
    "\n",
    "# 5. We also open the file for next tasks\n",
    "    als_points = als_file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264b9e65-fcb2-4828-a868-2b4e9cf29cb7",
   "metadata": {},
   "source": [
    "The first thing to note is that our data uses Point Record Format 6. If you're curious about the different point record formats, you can check out [this document](https://www.asprs.org/wp-content/uploads/2019/03/LAS_1_4_r14.pdf) (pages 15-36).\n",
    "\n",
    "Another interesting detail is that our dataset contains almost 23 million points! That’s a massive amount of data, right?\n",
    "\n",
    "Next, let’s double-check the format of our data by examining the dimensions available for each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7595339e-b301-4ce9-9b2e-f08382682aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to convert the object into a list for proper display\n",
    "list(als_points.point_format.dimension_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35ce949-5426-441c-aff8-44a152439a94",
   "metadata": {},
   "source": [
    "As we see, the information corresponds to the expected columns given the data format. Now, let's extract x, y and z and take a look at the first 5 points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05319157-f21a-4f94-98ab-f660084f1689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save XYZ as np arrays and print the first 5 points consecutively. \n",
    "x_als, y_als, z_als = als_points.x, als_points.y, als_points.z\n",
    "print(f\"First 5 ALS points (x, y, z): {list(zip(x_als[:5], y_als[:5], z_als[:5]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11670be4-f57c-41d2-82de-d808261d5735",
   "metadata": {},
   "source": [
    "## Visualizing the data\n",
    "\n",
    "Now that we have our data loaded into memory, let’s create a simple 3D visualization using [Plotly](https://plotly.com/python/). \n",
    "\n",
    "For this example, we’ll work with a sample of **50,000 points**. Visualizing the entire dataset would be too heavy for Jupyter, but this sample will give us a good idea of what the data looks like.\n",
    "\n",
    "If you’re curious to explore the full dataset, you can use dedicated software like [Cloud Compare](https://www.danielgm.net/cc/), which is specifically designed for handling large point clouds..  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7395467e-c547-4dc6-afd3-2c31dc90ea9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "subset_size = 50000  # You can change this value, but this sample should be enough for our visualization purposes.\n",
    "\n",
    "# 2. We create an interactive 3D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=x_als[:subset_size],\n",
    "    y=y_als[:subset_size],\n",
    "    z=z_als[:subset_size],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=2,                     \n",
    "        color=z_als[:subset_size],   # Color points by height (just to make it look nicer :D)\n",
    "        colorscale='Viridis',        \n",
    "        opacity=0.8                  \n",
    "    )\n",
    ")])\n",
    "\n",
    "# 3. We add a few details and show the figure\n",
    "fig.update_layout(\n",
    "    title=\"Airborne Laser Scanning (ALS) Point Cloud\",\n",
    "    scene=dict(\n",
    "        xaxis_title=\"X (m)\",\n",
    "        yaxis_title=\"Y (m)\",\n",
    "        zaxis_title=\"Z (m)\"\n",
    "    ),\n",
    "    margin=dict(l=0, r=0, b=0, t=40)  \n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f10c33-0b19-42a4-8c9f-4b11985dba59",
   "metadata": {},
   "source": [
    "Looks cool, right? As you can see, the visualization resembles the shape of a forest. If you zoom in, you’ll notice that each tree is made up of thousands of individual points, capturing fine details of the structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115b29aa-4e4b-49cc-970e-875b306deb66",
   "metadata": {},
   "source": [
    "## TLS Data\n",
    "\n",
    "Terrestrial Laser Scanning (TLS) is a ground-based LiDAR technique where sensors are stationed on tripods or vehicles to capture highly detailed 3D data of localized areas. Unlike ALS, which covers broad landscapes, TLS excels at capturing intricate structures, such as tree trunks, midstory vegetation, or buildings, with exceptional precision. TLS datasets often include coordinates (X, Y, Z), intensity values, and sometimes RGB colors, enabling detailed analysis of structural features. This technology is widely used in forestry for measuring tree dimensions, in architecture for building modeling, and in geology for mapping rock formations. \n",
    "\n",
    "## Our data\n",
    "\n",
    "The TLS files we’re working with are in the **Point Cloud Data Format (.ptx)**, and they’re quite large—each file is around 500MB. Downloading the entire set would be very inefficient, and processing files in this format directly with Python can be quite challenging.\n",
    "\n",
    "Thankfully, we have a tool called [PDAL](https://pdal.io/en/2.8.2/) that can help! PDAL allows us to convert our .ptx files into LAZ files without losing any information. Here’s the plan:\n",
    "\n",
    "1. Temporarily download the `.ptx` file to our server.\n",
    "2. Use PDAL to translate the `.ptx` file into a `.laz` file. (This is a terminal process, so we’ll need to use Python’s [subprocess](https://docs.python.org/3/library/subprocess.html) library.)\n",
    "3. Once the conversion is done, delete the original `.ptx` file to save space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a4c418-c904-498f-bdc2-a743e4330227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "# URL of the PTX file\n",
    "ptx_url = \"https://wifire-data.sdsc.edu/nc/s/5oiFgzCdo4QPi34/download?path=%2FLidar_data&files=CATNF_6041_20240814_1.ptx\"\n",
    "\n",
    "# Filenames - You can replace these names\n",
    "ptx_file = Path(\"input.ptx\")\n",
    "laz_file = Path(\"output.laz\")\n",
    "\n",
    "# 1. We request and download the .ptx file locally\n",
    "response = requests.get(ptx_url)\n",
    "\n",
    "with ptx_file.open(\"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "print(f\"Downloaded PTX file: {ptx_file}\")\n",
    "\n",
    "# 2. We convert the .ptx file to .laz using PDAL\n",
    "pdal_command = [\"pdal\", \"translate\", str(ptx_file), str(laz_file)]\n",
    "result = subprocess.run(pdal_command, capture_output=True, text=True)\n",
    "\n",
    "# This is a check in case there is some error with the transformation\n",
    "if result.returncode == 0:\n",
    "    print(f\"Converted {ptx_file} to {laz_file}\")\n",
    "else:\n",
    "    print(f\"PDAL translation failed: {result.stderr}\")\n",
    "    raise RuntimeError(f\"PDAL translation error: {result.stderr}\")\n",
    "\n",
    "# 3. We delete the original .ptx file\n",
    "ptx_file.unlink()\n",
    "print(f\"Deleted original PTX file: {ptx_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee80b71-c39f-475d-822e-e379a45989df",
   "metadata": {},
   "source": [
    "Success! 🎉 If you check the size of the `output.laz` file, you’ll notice it’s only around 25–30MB—much smaller than the original file. While we’ll delete it in the next cell, this compressed format will definitely make our lives easier as we move on to the next steps.\n",
    "\n",
    "Now, let's \"recycle\" our previous code to load the data using laspy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807a04bf-cfc2-4595-a84c-70fe9fb15087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the .las file using Laspy\n",
    "with laspy.open(laz_file) as tls_file:\n",
    "    print(f\"Point Format: {tls_file.header.point_format}\")\n",
    "    print(f\"Number of Points: {tls_file.header.point_count}\")\n",
    "\n",
    "    # Read all points from the LAZ file\n",
    "    tls_points = tls_file.read()\n",
    "    list(tls_points.point_format.dimension_names)\n",
    "\n",
    "# Now we can remove the .laz file\n",
    "laz_file.unlink()\n",
    "print(f\"Deleted original LAZ file: {laz_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd81398-ddb1-4eaa-a556-88ae30c6da23",
   "metadata": {},
   "source": [
    "The first thing we notice is that the format of this data is different from the previous one. Another key difference is that this file contains fewer points. Let’s take a closer look at the columns in this file to understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fadfe60-f29a-4ea6-a942-184d0e5ccbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We recycle code again :)\n",
    "list(tls_points.point_format.dimension_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190dae62-cf98-4188-8eb3-35f37522351d",
   "metadata": {},
   "source": [
    "One key difference between this dataset and the previous one is that it includes Red, Green, and Blue (RGB) colors. This additional information can be incredibly useful, especially for classification tasks, as it helps distinguish between different objects more effectively.\n",
    "\n",
    "Let's take a look at the first 5 data points of RGB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a2c14c-3c20-4f4f-ab9e-a51e4bbe202a",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_tls, g_tls, b_tls = tls_points.red, tls_points.green, tls_points.blue\n",
    "print(f\"\\nFirst 5 points (R, G, B): {list(zip(r_tls[:5], g_tls[:5], b_tls[:5]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc77592e-b249-4a54-ac0e-10e5e47385e3",
   "metadata": {},
   "source": [
    "Note: Since .ptx is a human-readable format, you can alternatively parse these files directly and load the data into an array using NumPy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a25f3ef-72d8-4f50-aa56-00f4358cf195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "with ptx_file.open(\"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "n_header = 10 # Number of rows in the header\n",
    "\n",
    "tls_ptx = np.loadtxt('input.ptx', skiprows=n_header)\n",
    "tls_ptx = tls_ptx[~np.all(tls_ptx == 0, axis=1)]\n",
    "print(tls_ptx.shape)\n",
    "\n",
    "ptx_file.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98550d3d-60ae-4328-be71-54aa047b8ab0",
   "metadata": {},
   "source": [
    "## Visualizing the data\n",
    "\n",
    "Just like we did with the ALS data, we’ll create a quick visualization of this TLS data. Keep in mind that since this data was captured from the ground, it’ll be harder to recognize the overall shape of the point clouds.\n",
    "\n",
    "For a more detailed and comprehensive visualization of this data, feel free to check out [this site](https://burnpro3d.sdsc.edu/points2pano/?plot=CATNF_6022&ts=20240731). The site was developed by the [WIFIRE](https://wifire.ucsd.edu/) team and uses TLS data to simulate a forest experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96133529-3208-41fd-af33-f4567a72e9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "subset_size = 100000  # Number of points to sample\n",
    "\n",
    "# We generate a set of random points\n",
    "random_indices = np.random.choice(len(tls_points.x), size=subset_size, replace=False)\n",
    "\n",
    "# Use the random indices to select points\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=tls_points.x[random_indices],\n",
    "    y=tls_points.y[random_indices],\n",
    "    z=tls_points.z[random_indices],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=2,\n",
    "        color=r_tls[random_indices],  # Let's use red as the scale for our colors\n",
    "        colorscale='Reds',\n",
    "        opacity=0.8\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"3D Point Cloud Visualization (LAZ Data)\",\n",
    "    scene=dict(\n",
    "        xaxis_title=\"X (m)\",\n",
    "        yaxis_title=\"Y (m)\",\n",
    "        zaxis_title=\"Z (m)\"\n",
    "    ),\n",
    "    margin=dict(l=0, r=0, b=0, t=40)  \n",
    ")\n",
    "\n",
    "# Display the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1269d1ea-3346-43b0-a615-cb0c3ec3341c",
   "metadata": {},
   "source": [
    "As we can observe, this visualization isn’t as easy to interpret as the earlier one. But if you look closely, you can start to spot the shape of the forest from the ground level, with the points coming together to form the tree trunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb919eb-78ba-4b51-8543-31d5a8be4b6d",
   "metadata": {},
   "source": [
    "## A simple application\n",
    "\n",
    "The ALS files we are working with include a column called classification, which provides a label for each point in the dataset. These labels represent the category of each point, such as ground, vegetation, building, and others. This classification is critical for interpreting point cloud data and is often used in applications like terrain mapping, vegetation analysis, and urban planning.\n",
    "\n",
    "The classification process is typically performed using a combination of manual annotation by experts and automated methods involving sophisticated Machine Learning models. Pages 17-19 of [this document](https://www.asprs.org/wp-content/uploads/2019/03/LAS_1_4_r14.pdf) provide detailed information about the classification standards and their specific meanings.\n",
    "\n",
    "For this demo, we will take a sample of points and use it to build a simple point cloud classifier. Our goal is to leverage the available data to train a model capable of predicting the class of a point based on its attributes. \n",
    "\n",
    "As you might expect, processing and working with ALS and TLS data presents significant challenges due to the sheer size of the datasets. To handle this effectively, we need to leverage specialized libraries and resources. For this module, we’ve chosen Dask to process the data and train a simple machine learning model.\n",
    "\n",
    "Why Dask? Dask is designed for parallel and distributed computing, making it an excellent choice for large-scale data processing. It efficiently utilizes all available CPU cores by splitting data into manageable chunks and processing them in parallel.\n",
    "\n",
    "In the instructions for this module, you were asked to reserve 2 cores. Let’s start by confirming the availability of these resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac6fe5a-5726-4ac5-9c93-6517a84497ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.system import cpu_count\n",
    "\n",
    "cores = cpu_count()\n",
    "print(f\"Number of CPU cores available: {cores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9ba977-00ed-41b7-978f-41292cbb6144",
   "metadata": {},
   "source": [
    "Great! We have 2 cores available. \n",
    "\n",
    "Another key aspect of our setup is the allocation of 64GB of memory. This provides ample resources for libraries like xgboost (which we will use for training) to efficiently process data and perform computations directly in memory, ensuring reliable performance during training.\n",
    "\n",
    "***NOTE***: *In this module, we selected Dask as an example of a library well-suited for handling big data. However, with the current setup using only 2 cores, processing the massive datasets generated by both ALS and TLS remains challenging. For the data challenge, we encourage you to explore alternative libraries and frameworks that align better with your specific requirements. \n",
    "To do this effectively, you'll need to configure the appropriate environment within the NDP JupyterHub. We recommend reviewing the provided [documentation](https://nationaldataplatform.org/documentation/jupyter/bring-your-own-image/), which offers step-by-step instructions for building your own Docker image, including the one used in this demo.*\n",
    "\n",
    "Now, lets build a simple classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11395db4-c4ee-4e82-95b3-fa87d3852dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "\n",
    "# 1. First, we will transform our features into Dask arrays\n",
    "x_coords = da.from_array(als_points.x, chunks=10000)\n",
    "y_coords = da.from_array(als_points.y, chunks=10000)\n",
    "z_coords = da.from_array(als_points.z, chunks=10000)\n",
    "intensity = da.from_array(als_points.intensity, chunks=10000)\n",
    "classification = da.from_array(als_points.classification, chunks=10000)  # This is the target variable\n",
    "\n",
    "# 2. Now, we define sample size and generate random indices\n",
    "sample_size = 2_000_000  # For this example, we will only use a sample of 2,000,000 points, otherwise training time would be more extensive.\n",
    "total_size = x_coords.shape[0]\n",
    "random_indices_np = np.random.choice(total_size, size=sample_size, replace=False)\n",
    "random_indices = da.from_array(random_indices_np, chunks=10000).compute()\n",
    "\n",
    "# 3. Now we convert Dask arrays to NumPy arrays for selected samples\n",
    "x_sample = x_coords.compute()[random_indices]\n",
    "y_sample = y_coords.compute()[random_indices]\n",
    "z_sample = z_coords.compute()[random_indices]\n",
    "intensity_sample = intensity.compute()[random_indices]\n",
    "classification_sample = classification.compute()[random_indices]\n",
    "\n",
    "# 4. We stack all features in a single array for easier processing\n",
    "features = np.stack([x_sample, y_sample, z_sample, intensity_sample], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4948c612-f760-4efd-8ba7-37fdcc8fae6d",
   "metadata": {},
   "source": [
    "One thing about this data, is the high unbalance between class 1 and the rest of the classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56478d6-a934-4b90-bc7f-517a2eb6c433",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class_counts = Counter(als_points.classification)\n",
    "total_points = sum(class_counts.values())\n",
    "\n",
    "for cls, count in class_counts.items():\n",
    "    percentage = (count / total_points) * 100\n",
    "    print(f\"Class {cls}: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dc9270-004c-4ea3-b860-69e927c84196",
   "metadata": {},
   "source": [
    "Class 1 represents around 73% of the total points. Therefore, we need to account for that in our training design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cbd602-e5f6-47f4-9cdd-52abe4e574a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# 5. We perform a stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, classification_sample, stratify=classification_sample, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 6. Then, we map non-contiguous class labels to contiguous ones - This is a required step for XGBoost\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_mapped = label_encoder.fit_transform(y_train)\n",
    "y_test_mapped = label_encoder.transform(y_test)\n",
    "\n",
    "# 7. Now, we apply SMOTE to oversample minority classes, otherwise our model will struggle to learn\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train_mapped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5109f1f-be23-4afc-8086-fa29b2a8d447",
   "metadata": {},
   "source": [
    "With our data prepared and the class imbalances accounted for, we can now proceed with the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1757eba-ff0b-458f-842d-84339de6c926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FYI This cell will take around 3 min to run\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# 8. With the data ready, we initialize and train the XGBoost model\n",
    "model = XGBClassifier(\n",
    "    objective=\"multi:softmax\",  # Multi-class classification\n",
    "    num_class=len(np.unique(y_resampled)),  # Number of classes\n",
    "    random_state=42,\n",
    "    eval_metric=\"mlogloss\"\n",
    ")\n",
    "model.fit(X_resampled, y_resampled)\n",
    "\n",
    "# 9. After training, we predict on the test set\n",
    "y_pred_mapped = model.predict(X_test)\n",
    "y_pred = label_encoder.inverse_transform(y_pred_mapped) # We return the original labels to our classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e346950a-b7c0-4919-8cb8-527ec670d19f",
   "metadata": {},
   "source": [
    "Success! Our model is now trained and ready to be evaluated. We will evaluate using the accuracy score, a classification report and printing the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f522ff-53f1-4ac9-b5d1-4a6f92f28e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# 10. Now we have to evaluate the model\n",
    "print(f\"Accuracy Score: {accuracy_score(y_test, y_pred) * 100:.2f}%\\n\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd41c72-62f9-49ec-b0ef-de18ac2450cc",
   "metadata": {},
   "source": [
    "The classifier demonstrates decent performance, achieving an accuracy of 71.50%. However, it exhibits low precision for labels 7 and 20, which correspond to noise points and ignored ground, respectively. This outcome is expected, as these labels are underrepresented in our dataset, making it more challenging for the model to learn and predict them accurately.\n",
    "\n",
    "As we discussed earlier, a classifier like this can serve as a valuable tool to assist in the classification of points captured in the field. While this example focuses on simplicity, it’s worth noting that the classifier could be significantly improved by incorporating more advanced techniques such as feature engineering, ensemble methods, or deep learning.\n",
    "\n",
    "For the purposes of this demo, however, we have prioritized a straightforward approach to illustrate the foundational concepts behind building and evaluating a point cloud classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ddc1d2-012b-46f1-be3b-cb3306399ff2",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "In this module, we’ve provided a brief introduction to ALS (Airborne Laser Scanning) and TLS (Terrestrial Laser Scanning) data. Your challenge will be to identify a meaningful use case for this data and apply it to develop more effective fire models.\n",
    "\n",
    "To wrap up this module, we encourage you to dive deeper by exploring the following resources:\n",
    "\n",
    "- https://www.usgs.gov/faqs/what-lidar-data-and-where-can-i-download-it\n",
    "- https://oceanservice.noaa.gov/geodesy/lidar.html\n",
    "- https://research.fs.usda.gov/srs/products/compasslive/measuring-forest-structure-lidar-and-finding-right-resolution\n",
    "- https://sweri.eri.nau.edu/wp-content/uploads/2019/11/FactSheet_LiDAR_Oct2019_FINAL.pdf\n",
    "- https://research.fs.usda.gov/treesearch/30652"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
